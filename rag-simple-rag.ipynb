{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Simple RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "## What is RAG? \n",
    "\n",
    "- Aka Retrieval Augmented Generation: Is an industry pattern standard for building applications that use Large Language Models (LLMs) to reason over specific or proprietary data not already known by LLMs. Simply put, RAG enhances the capabilities of LLMs by combining retrieval mechanisms with generation mechanisms\n",
    "\n",
    "## Why create a RAG?\n",
    "\n",
    "### Key intentions and Benefits of RAG:\n",
    "1. ## Enhance knowledge context\n",
    "    - Access to external information: \n",
    "    - Context Relevance: \n",
    "\n",
    "2. ### Improved Response quality:\n",
    "    - Accuracy and specificity: \n",
    "    - Diversity of information:\n",
    "\n",
    "3. ### Scalability and adaptability:\n",
    "    - Dynamic updates: RAG systems can easily incorporate new data and information without requiring extensive retraining of the underlying language  model. This makes it adaptable to new information and rapidly changing fields.\n",
    "    - Domain specific knowledge: By indexing domain-specific documents, RAG systems can be tailored to provide expertise in particular areas, such as medical, legal, technical, financial, or other specialized domains.\n",
    "\n",
    "4. ### Efficiency and performance:\n",
    "    - Reduced model size requirements: RAG can reduce the need for for extremely LLMs since the retrieval mechanism can offload some of the knowledge storage and processing to the retrieval system.\n",
    "    - Focused Computation: Instead of generating responses solely from the model's internal knowledge, RAG narrows down the relevant information, making the generation process more focused and efficient.\n",
    "\n",
    "5. ### Use cases and applications\n",
    "    1. Question answering: RAG is great for Q&A systems where specific accurate answers are required\n",
    "    2. Customer support: Chat-bots capabilitIes to provide precise information by retrieving relevant documentation or FAQs\n",
    "    3. Research assistance: Retrieving pertinent literature and generation summaries or insights based on the latest studies\n",
    "    4. Content creation: Supporting writers and content creators by retrieving related content and generating high-quality text based on that information\n",
    "    \n",
    "\n",
    "## How RAG Works?\n",
    "\n",
    "1. Query processing \n",
    "    - A user query is processed to generate an embedding or vector representation of the query\n",
    "2. Document Retrieval:\n",
    "    - The query embedding is used to retrieve relevant documents or passages from the vector database using similarity search techniques.\n",
    "3. Contextual Generation:\n",
    "    - The retrieved documents are combined with the original query to form a rich context.\n",
    "    - The language model generates a response based on this context, leveraging both the retrieved information and its pre-trained knowledge.\n",
    "\n",
    "\n",
    "## Example Workflow\n",
    "\n",
    "1. Input query: \"What are the latest advancements in AI for healthcare?\"\n",
    "2. Retrieval Step: Retrieve documents on AI advancements in healthcare from a knowledge base or database.\n",
    "3. Generation step: Use the retrieved documents to generate a detailed and accurate response. \n",
    "\n",
    "\n",
    "# Creating a simple Retrieval-Augmented \n",
    "\n",
    "Lets build a simple RAG system\n",
    "\n",
    "Creating a RAG system with OpenAI/others involves several steps: \n",
    "- Preparing your data\n",
    "- Setting up vector database\n",
    "- Generating embeddings\n",
    "- Retrieving relevant documents\n",
    "- Generating responses\n",
    "\n",
    "## Step 1: Preparing your data\n",
    "\n",
    "Collect and pre-process your data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"AI is transforming healthcare by enabling doctors to diagnose diseases more accurately.\",\n",
    "    \"Machine learning algorithms can predict patient outcomes and suggest personalized treatments.\",\n",
    "    \"Natural language processing is being used to analyze medical records and improve patient care.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set Up Embedding Generation\n",
    "\n",
    "Use OpenAI's GPT-3 or other model to generate embeddings for your documents. You'll need OpenAI's API key for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade openai numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01953129  0.02010844  0.01069696 ... -0.00245125 -0.01942636\n",
      "   0.00184131]\n",
      " [-0.01801853  0.00049462  0.02333334 ... -0.00171759 -0.01698149\n",
      "   0.00535695]\n",
      " [-0.02200545  0.02710145  0.01247619 ...  0.00448152 -0.0134349\n",
      "  -0.0170124 ]]\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# openai.api = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI()\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    response = client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "    return response\n",
    "\n",
    "# Generate embeddings for each document\n",
    "\n",
    "embeddings = [get_embedding(doc) for doc in documents]\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Setting up vector database\n",
    "\n",
    "For simplicity, we will use FAISS, a library for efficient similarity search and clustering of dense vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pynndescent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/learnwithcosmos/repos/nlp/workflow/.venv/lib/python3.12/site-packages/pynndescent/pynndescent_.py:962: UserWarning: Failed to correctly find n_neighbors for some samples. Results may be less than ideal. Try re-running with different parameters.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices [[0 1 2 2 2]]\n",
      "Distances [[0.         0.11787299 0.12762608 1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import pynndescent\n",
    "\n",
    "# Create a nearest neighbors index\n",
    "\n",
    "index = pynndescent.NNDescent(embeddings, metric='cosine')\n",
    "\n",
    "# Find the nearest neighbors of the first document\n",
    "\n",
    "query_embedding = embeddings[0].reshape(1, -1)\n",
    "indices, distances = index.query(query_embedding, k=5)\n",
    "\n",
    "# Print the nearest neighbors\n",
    "\n",
    "print(\"Indices\", indices)\n",
    "print(\"Distances\", distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define the Retrieval Function\n",
    "Create a function to retrieve the most relevant documents based on a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, k=3):\n",
    "    query_embedding = np.array([get_embedding(query)])\n",
    "    query_embedding = embeddings[0].reshape(1, -1)\n",
    "    indices, distances = index.query(query_embedding, k=k)\n",
    "    return [documents[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define the Generation Function\n",
    "Create a function to generate responses using the retrieved documents as context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test the RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI is utilized in healthcare in various ways, such as enabling doctors to diagnose diseases more accurately, predicting patient outcomes using machine learning algorithms, suggesting personalized treatments, and analyzing medical records to improve patient care through natural language processing. Additionally, AI is also being used for tasks like drug discovery, robotic surgeries, remote patient monitoring, and personalized medicine.\n"
     ]
    }
   ],
   "source": [
    "def generate_response(query):\n",
    "    retrieved_docs = retrieve(query)\n",
    "    context = \" \".join(retrieved_docs)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"}\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        max_tokens=150,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# Test the RAG system\n",
    "query = \"How is AI used in healthcare?\"\n",
    "response = generate_response(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "\n",
    "pip install --upgrade openai numpy pynndescent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "from openai import  OpenAI\n",
    "import numpy as np\n",
    "import pynndescent\n",
    "\n",
    "# 1. Set the OpenAI API key\n",
    "openai = OpenAI()\n",
    "\n",
    "# 2. Document sources\n",
    "documents = [\n",
    "    \"AI is transforming healthcare by enabling doctors to diagnose diseases more accurately.\",\n",
    "    \"Machine learning algorithms can predict patient outcomes and suggest personalized treatments.\",\n",
    "    \"Natural language processing is being used to analyze medical records and improve patient care.\",\n",
    "]\n",
    "\n",
    "# 3. Define a function to get embeddings\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    response = client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "    return response\n",
    "\n",
    "# 4. Generate embeddings for each document\n",
    "embeddings = np.array([get_embedding(doc) for doc in documents])\n",
    "\n",
    "# 5. Create a nearest neighbors index using pynndescent\n",
    "index = pynndescent.NNDescent(embeddings, metric='cosine')\n",
    "\n",
    "# 6. Define a function to retrieve relevant documents\n",
    "def retrieve(query, k=3):\n",
    "    query_embedding = np.array([get_embedding(query)])\n",
    "    indices, distances = index.query(query_embedding, k=k)\n",
    "    return [documents[i] for i in indices[0]]\n",
    "\n",
    "# 7. Define a function to generate a response using OpenAI's Chat API\n",
    "def generate_response(query):\n",
    "    retrieved_docs = retrieve(query)\n",
    "    context = \" \".join(retrieved_docs)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a Healthcare AI expert.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"}\n",
    "    ]\n",
    "    \n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        max_tokens=150,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# 8. Test the RAG system\n",
    "queries = [\n",
    "    \"How is AI used in healthcare?\",\n",
    "    \"What are the applications of AI in healthcare?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    response = generate_response(query)\n",
    "    print(f\"Query: {query}\\nResponse: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "1. **Imports and Setup**:\n",
    "   - Import necessary modules: `openai`, `numpy`, and `pynndescent`.\n",
    "   - Initialize OpenAI\n",
    "\n",
    "2. **Document Sources**:\n",
    "   - Define a list of documents that will be used as the source material for the RAG system.\n",
    "\n",
    "3. **Embedding Generation**:\n",
    "   - Define a function `get_embedding` to generate embeddings using OpenAI's embedding model.\n",
    "   - The function calls OpenAI's `embedding.create` and returns the embedding for the given text.\n",
    "\n",
    "4. **Generate Embeddings**:\n",
    "   - Generate embeddings for each document in the list using the `get_embedding` function and store them in a numpy array.\n",
    "\n",
    "5. **Simple vector (Replace with actual vector database): Nearest Neighbors Index**:\n",
    "   - Create a nearest neighbors index using `pynndescent` for efficient document retrieval based on embeddings.\n",
    "\n",
    "6. **Document Retrieval**:\n",
    "   - Define a function `retrieve` to find and return the most relevant documents for a given query.\n",
    "   - The function uses the `index.query` method to find the nearest neighbors of the query embedding.\n",
    "\n",
    "7. **Response Generation**:\n",
    "   - Define a function `generate_response` to generate a response using OpenAI's Chat API.\n",
    "   - The function combines the retrieved documents into a context and constructs a prompt for the chat model.\n",
    "   - It calls `openai.chat.completion.create` to generate a response.\n",
    "\n",
    "8. **Testing the RAG System**:\n",
    "   - Test the RAG system with a list of queries and print the responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (Retrieval-Augmented Generation) Architecture\n",
    "\n",
    "```mermaid\n",
    "graph TD;\n",
    "    A[Set OpenAI API Key] --> B[Document Sources];\n",
    "    B --> C[Define Function to Get Embeddings];\n",
    "    C --> D[Generate Embeddings for Each Document];\n",
    "    D --> E[Create Nearest Neighbors Index Using pynndescent];\n",
    "    E --> F[Define Function to Retrieve Relevant Documents];\n",
    "    F --> G[Define Function to Generate a Response Using OpenAI's Chat API];\n",
    "    G --> H[Test the RAG System];\n",
    "\n",
    "    subgraph Retrieve-Generate Cycle\n",
    "        F --> G\n",
    "    end\n",
    "\n",
    "    subgraph Testing Queries\n",
    "        H --> I[How is AI used in healthcare?];\n",
    "        H --> J[What are the applications of AI in healthcare?];\n",
    "    end\n",
    "\n",
    "    I --> K[Generate Response for Query];\n",
    "    J --> K[Generate Response for Query];\n",
    "\n",
    "    K --> L[Print Response];\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade openai numpy pynndescent portkey_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building a Retrieval-Augmented Generation (RAG) system involves integrating an information retrieval component with a language generation model. The retrieval component aims to find relevant information from a large corpus of text, while the generation model uses this retrieved information to generate natural language responses. Here's a general approach to building a RAG system and streamlining the process:\n",
      "\n",
      "1. **Data Preparation**:\n",
      "   - Acquire or create a large corpus of text data relevant to your domain or task.\n",
      "   - Clean and preprocess the text data, including tasks like tokenization, normalization, and filtering.\n",
      "   - Index the text data using an efficient information retrieval system like Apache Lucene, Elasticsearch, or a custom solution.\n",
      "\n",
      "2. **Retrieval Component**:\n",
      "   - Implement a retrieval mechanism that can efficiently query the indexed corpus and retrieve relevant passages or documents based on the input query or context.\n",
      "   - Explore techniques like sparse retrieval (e.g., BM25, TF-IDF), dense retrieval (e.g., dense passage embeddings), or a combination of both.\n",
      "   - Consider incorporating techniques like query expansion, re-ranking, and document filtering to improve retrieval quality.\n",
      "\n",
      "3. **Language Generation Model**:\n",
      "   - Choose a suitable language generation model, such as GPT-3, BART, T5, or a custom transformer-based model.\n",
      "   - Fine-tune the model on a task-specific dataset, incorporating the retrieved information as additional input.\n",
      "   - Explore techniques like concatenating retrieved passages, using attention mechanisms, or conditioning the model on the retrieved information.\n",
      "\n",
      "4. **Iterative Refinement**:\n",
      "   - Evaluate the performance of the RAG system using appropriate metrics (e.g., ROUGE, BLEU, task-specific metrics).\n",
      "   - Iteratively refine the retrieval and generation components based on the evaluation results.\n",
      "   - Experiment with different retrieval techniques, generation models, and fine-tuning strategies to improve performance.\n",
      "\n",
      "5. **Optimization and Deployment**:\n",
      "   - Optimize the retrieval and generation components for efficiency, considering techniques like caching, pruning, and distributed computing.\n",
      "   - Implement proper serving infrastructure and APIs for deploying the RAG system in production environments.\n",
      "   - Consider integrating the RAG system with other components\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders\n",
    "\n",
    "gateway = OpenAI(\n",
    "    api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "    base_url=\"http://localhost:8787/v1\", # Or http://localhost:8787/v1 when running locally\n",
    "    default_headers=createHeaders(\n",
    "        provider=\"anthropic\",\n",
    "        # api_key=\"port_key\" # Grab from https://app.portkey.ai # Not needed when running locally\n",
    ")\n",
    "\n",
    "chat_complete = gateway.chat.completions.create(\n",
    "    model=\"claude-3-sonnet-20240229\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the best way to build a RAG system and streamline the process?\"}],\n",
    "    max_tokens=512\n",
    ")\n",
    "\n",
    "print(chat_complete.choices[0].message.content.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
